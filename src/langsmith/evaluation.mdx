---
title: Evaluation
sidebarTitle: Overview
mode: wide
---

Welcome to the LangSmith Evaluation documentation. The following sections help you create datasets, run evaluations, and analyze results:

- **Datasets**: [Create](/langsmith/manage-datasets-in-application) and [manage](/langsmith/manage-datasets) datasets for evaluation, including creating datasets through the UI or SDK and managing existing datasets.

- **Evaluations**: [Run evaluations](/langsmith/evaluate-llm-application) on your applications using various methods and techniques, including different evaluator types and evaluation techniques.

- **Analyze experiment results**: [View and analyze your evaluation results](/langsmith/analyze-an-experiment), including comparing experiments, filtering results, and downloading data.

- **Annotation & human feedback**: Collect human feedback on your application outputs through [annotation queues](/langsmith/annotation-queues) and [inline annotation](/langsmith/annotate-traces-inline).

- **Tutorials**: Follow step-by-step tutorials to evaluate different types of applications, from [chatbots](/langsmith/evaluate-chatbot-tutorial) to [complex agents](/langsmith/evaluate-complex-agent).

For terminology definitions and core concepts, refer to the [introduction on evaluation](/langsmith/evaluation-concepts).
