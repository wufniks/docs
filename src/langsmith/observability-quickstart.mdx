---
title: Observability quickstart
sidebarTitle: Quickstart
---

Observability is important for any software application, but especially so for LLM applications. LLMs are **non-deterministic** by nature, meaning they can **produce unexpected results**. This makes them trickier than normal to debug.

This is where LangSmith can help! LangSmith gives you **visibility into each step** your application takes when handling a request â€” helping you **debug faster and gain confidence** in your app. From prototyping to production, LangSmith has you covered with **tracing,  filtering, charting, and alerting** to keep your application reliable at scale.

# Get started

This tutorial will show you how to instrument a simple [RAG](https://www.mckinsey.com/featured-insights/mckinsey-explainers/what-is-retrieval-augmented-generation-rag) application that consists of a retrieval step to fetch data and an LLM call to OpenAI to answer the user question based on the data. 

<Tip>
  If you're building an application with [LangChain](https://python.langchain.com/docs/introduction/) or [LangGraph](https://langchain-ai.github.io/langgraph/), get started by reading the guides for tracing with [LangChain](/langsmith/trace-with-langchain) or tracing with [LangGraph](/langsmith/trace-with-langgraph).
</Tip>

## 1. Install Dependencies

<Tabs>
  <Tab title="Python">

```bash
pip install -U langsmith openai
```

</Tab>
  <Tab title="TypeScript">

```bash
yarn add langsmith openai
```

</Tab>
</Tabs>

## 2. Create an API key

To create an API key head to the [LangSmith settings page](https://smith.langchain.com/settings). Then click **+ API Key.**

## 3. Set up environment variables

This example uses OpenAI, but you can adapt it to use any LLM provider. 

If you're using Anthropic, use the [Anthropic wrapper](/langsmith/annotate-code#wrap-the-anthropic-client-python-only) to trace your calls. For other providers, use [the traceable wrapper](/langsmith/annotate-code#use-%40traceable-%2F-traceable).

``` bash
export LANGSMITH_TRACING=true
export LANGSMITH_API_KEY="<your-langsmith-api-key>"
export OPENAI_API_KEY="<your-openai-api-key>"
```

## 4. Define your application

We will instrument a simple RAG application for this tutorial, but feel free to use your own code if you'd like - just make sure it has an LLM call!

<Accordion title="Application Code">
  <Tabs>
    <Tab title="Python">

```python
from openai import OpenAI
openai_client = OpenAI()

# This is the retriever we will use in RAG
# This is mocked out, but it could be anything we want
def retriever(query: str):
    results = ["Harrison worked at Kensho"]
    return results

# This is the end-to-end RAG chain.
# It does a retrieval step then calls OpenAI
def rag(question):
    docs = retriever(question)
    system_message = """Answer the users question using only the provided information below:
        {docs}""".format(docs="\n".join(docs))
        
    return openai_client.chat.completions.create(
        messages=[
            {"role": "system", "content": system_message},
            {"role": "user", "content": question},
        ],
        model="gpt-4o-mini",
    )
```

</Tab>
    <Tab title="TypeScript">

```typescript
import { OpenAI } from "openai";

const openAIClient = new OpenAI();

// This is the retriever we will use in RAG
// This is mocked out, but it could be anything we want
async function retriever(query: string) {
  return ["This is a document"];
}

// This is the end-to-end RAG chain.
// It does a retrieval step then calls OpenAI
async function rag(question: string) {
  const docs = await retriever(question);
  
  const systemMessage =
    "Answer the users question using only the provided information below:\n\n" +
    docs.join("\n");
    
  return await openAIClient.chat.completions.create({
    messages: [
      { role: "system", content: systemMessage },
      { role: "user", content: question },
    ],
    model: "gpt-4o-mini",
  });
}
```

</Tab>
  </Tabs>
</Accordion>

## 5. Trace OpenAI calls

The first thing you might want to trace is all your OpenAI calls. LangSmith makes this easy with the [`wrap_openai`](https://docs.smith.langchain.com/reference/python/wrappers/langsmith.wrappers._openai.wrap_openai) (Python) or [`wrapOpenAI`](https://docs.smith.langchain.com/reference/js/functions/wrappers_openai.wrapOpenAI) (TypeScript) wrappers. All you have to do is modify your code to use the wrapped client instead of using the `OpenAI` client directly.

<Tabs>
  <Tab title="Python">

```python
from openai import OpenAI
from langsmith.wrappers import wrap_openai

openai_client = wrap_openai(OpenAI())

# This is the retriever we will use in RAG
# This is mocked out, but it could be anything we want
def retriever(query: str):
    results = ["Harrison worked at Kensho"]
    return results

# This is the end-to-end RAG chain.
# It does a retrieval step then calls OpenAI
def rag(question):
    docs = retriever(question)
    system_message = """Answer the users question using only the provided information below:
        {docs}""".format(docs="\n".join(docs))
        
    return openai_client.chat.completions.create(
        messages=[
            {"role": "system", "content": system_message},
            {"role": "user", "content": question},
        ],
        model="gpt-4o-mini",
    )
```

</Tab>
  <Tab title="TypeScript">

```typescript
import { OpenAI } from "openai";
import { wrapOpenAI } from "langsmith/wrappers";

const openAIClient = wrapOpenAI(new OpenAI());

// This is the retriever we will use in RAG
// This is mocked out, but it could be anything we want
async function retriever(query: string) {
  return ["This is a document"];
}

// This is the end-to-end RAG chain.
// It does a retrieval step then calls OpenAI
async function rag(question: string) {
  const docs = await retriever(question);
  
  const systemMessage =
    "Answer the users question using only the provided information below:\n\n" +
    docs.join("\n");
    
  return await openAIClient.chat.completions.create({
    messages: [
      { role: "system", content: systemMessage },
      { role: "user", content: question },
    ],
    model: "gpt-4o-mini",
  });
}
```

</Tab>
</Tabs>

Now when you call your application as follows:

```
rag("where did harrison work")
```

This will produce a trace of just the OpenAI call in LangSmith's default tracing project. It should look something like [this](https://smith.langchain.com/public/e7b7d256-10fe-4d49-a8d5-36ca8e5af0d2/r).

![](/langsmith/images/tracing-tutorial-openai.png)

## 6. Trace entire application

You can also use the `traceable` decorator ([Python](https://docs.smith.langchain.com/reference/python/run_helpers/langsmith.run_helpers.traceable) or [TypeScript](https://langsmith-docs-bdk0fivr6-langchain.vercel.app/reference/js/functions/traceable.traceable)) to trace your entire application instead of just the LLM calls.

<Tabs>
  <Tab title="Python">

```python
from openai import OpenAI
from langsmith import traceable
from langsmith.wrappers import wrap_openai

openai_client = wrap_openai(OpenAI())

def retriever(query: str):
    results = ["Harrison worked at Kensho"]
    return results

@traceable
def rag(question):
    docs = retriever(question)
    system_message = """Answer the users question using only the provided information below:
        {docs}""".format(docs="\n".join(docs))
        
    return openai_client.chat.completions.create(
        messages=[
            {"role": "system", "content": system_message},
            {"role": "user", "content": question},
        ],
        model="gpt-4o-mini",
    )
```

</Tab>
  <Tab title="TypeScript">

```typescript
import { OpenAI } from "openai";
import { traceable } from "langsmith/traceable";
import { wrapOpenAI } from "langsmith/wrappers";

const openAIClient = wrapOpenAI(new OpenAI());

async function retriever(query: string) {
  return ["This is a document"];
}

const rag = traceable(async function rag(question: string) {
  const docs = await retriever(question);
  
  const systemMessage =
    "Answer the users question using only the provided information below:\n\n" +
    docs.join("\n");
    
  return await openAIClient.chat.completions.create({
    messages: [
      { role: "system", content: systemMessage },
      { role: "user", content: question },
    ],
    model: "gpt-4o-mini",
  });
});
```

</Tab>
</Tabs>

Now if you call your application as follows:

```
rag("where did harrison work")
```

This will produce a trace of just the entire pipeline (with the OpenAI call as a child run) - it should look something like [this](https://smith.langchain.com/public/2174f4e9-48ab-4f9e-a8c4-470372d976f1/r)

![](/langsmith/images/tracing-tutorial-chain.png)

## Next steps

Congratulations! If you've made it this far, you're well on your way to being an expert in observability with LangSmith. Here are some topics you might want to explore next:

* [Tracing Integrations](/langsmith/trace-with-langchain)
* [Send traces to a specific project](/langsmith/log-traces-to-project)
* [Filter traces in a project](/langsmith/filter-traces-in-application)

If you prefer a video tutorial, check out the [Tracing Basics video](https://academy.langchain.com/pages/intro-to-langsmith-preview) from the Introduction to LangSmith Course.
