---
title: Overview
---

import AlphaCallout from '/snippets/alpha-lc-callout.mdx';

<AlphaCallout />


LangChain is the easiest way to start building with LLMs, letting you get started on building agents with OpenAI, Anthropic, Google and more in under 10 lines of code.

LangChain agents are built on top of [LangGraph](/oss/langgraph/overview). This is done to take advantage of the capabilities LangGraph provides: durable execution, streaming, human-in-the-loop, persistence. You do not need to know LangGraph for basic LangChain agent usage, but some advanced functionality may require LangGraph knowledge.

Install LangGraph:

:::python
```python
pip install -U langgraph
```
:::

:::js
```js
npm install @langchain/langgraph @langchain/core
```
:::

Then, create an agent [using prebuilt components](/oss/langgraph/prebuilts):

:::python
```python
# pip install -qU "langchain[anthropic]" to call the model

from langgraph.prebuilt import create_react_agent

def get_weather(city: str) -> str:
    """Get weather for a given city."""
    return f"It's always sunny in {city}!"

agent = create_react_agent(
    model="anthropic:claude-3-7-sonnet-latest",
    tools=[get_weather],
    prompt="You are a helpful assistant"
)

# Run the agent
agent.invoke(
    {"messages": [{"role": "user", "content": "what is the weather in sf"}]}
)
```
:::

:::js
```js
// npm install @langchain/anthropic to call the model

import { createReactAgent } from "@langchain/langgraph/prebuilt";
import { initChatModel } from "langchain/chat_models/universal";
import { tool } from "@langchain/core/tools";
import { z } from "zod";

const getWeather = tool( 
  async (input: { city: string }) => {
    return `It's always sunny in ${input.city}!`;
  },
  {
    name: "getWeather",
    schema: z.object({
      city: z.string().describe("The city to get the weather for"),
    }),
    description: "Get weather for a given city.",
  }
);

const llm = await initChatModel("anthropic:claude-3-7-sonnet-latest"); 
const agent = createReactAgent({
  llm,
  tools: [getWeather], 
  prompt: "You are a helpful assistant", 
});

// Run the agent
await agent.invoke({
  messages: [{ role: "user", content: "what is the weather in sf" }],
});

```
:::


## Core benefits

* [Standard Model Interface](/langchain/models): Different model providers have different APIs for interacting with models, including the format of the [messages](/langchain/messages) they return. LangChain standardizes how you call and interact with models. This allows you to seamlessly swap across model providers and avoid lock in.

* [Easy to use, yet highly flexible agent](/langchain/agents): LangChain's agent abstraction is designed to be easy to get started with - letting you build a simple agent in ~10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.

* [Built on top of LangGraph](/oss/langgraph/overview): LangChain's agents are built on top of LangGraph. This allows us to take advantage of LangGraph's durable execution, human-in-the-loop support, persistence, and more.

* [Debugging with LangSmith](/langsmith/overview): Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.